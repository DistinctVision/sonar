## Однородные координаты.  
В предыдущей статье мы рассматривали эвклидово пространство. 
Точки в двумерном эвклидовом пространстве обозначаются при помощи двумерных векторов $\begin{pmatrix} x & y \end{pmatrix}^T$. А такое пространство обозначается как $\mathbb{R}^2$. $\begin{pmatrix} x & y \end{pmatrix}^T \in \mathbb{R}^2$.  
Введем понятие проективного пространства. Оно обозначается как $\mathbb{P}^2$ (projective
space). Чтобы перевести вектор из двумерного эвклидового пространства и перевести его в проективное, нужно взять любой ненулевой скаляр, умножить на него вектор и добавить этот скаляр в качестве последней компоненты: $\vec v = \begin{pmatrix} x & y \end{pmatrix}^T \in \mathbb{R}^2 \space \rightarrow \space \vec p = \begin{pmatrix} x \cdot w & y \cdot w & w \end{pmatrix}^T \in \mathbb{P}^2$. Получаем координаты точки в уже в однородных координатах (homogeneous coordinates) проективного пространства. Таким образом одной точке проективного пространства принадлежит бесконечное множество точек эвклидового пространства. Чтобы перевести однородные координаты точки обратно в эвклидово простраство, нужно соответственно разделить вектор на его последнюю компоненту и удалить ее же из вектора: $\vec p = \begin{pmatrix} x & y & w \end{pmatrix}^T \in \mathbb{P}^2 \space \rightarrow \space \vec v = \begin{pmatrix} \frac{x}{w} & \frac{y}{w} \end{pmatrix}^T \in \mathbb{R}^2$.  
Тут же можно заметить, что умножение однородных координат на скаляр не меняет координат соответствующих точек в эвклидовом пространстве: $\vec{p_B} = \vec{p_A} \cdot s = \begin{pmatrix} x \cdot s & y \cdot s & w \cdot s \end{pmatrix}^T \in \mathbb{P}^2 \space \rightarrow \space \vec v = \begin{pmatrix} \frac{x \cdot s}{w \cdot s} & \frac{y \cdot s}{w \cdot s} \end{pmatrix}^T = \begin{pmatrix} \frac{x}{w} & \frac{y}{w} \end{pmatrix}^T \in \mathbb{R}^2$.  
Также, если последняя компонента равна 0, то такую точку мы не сможем перевести в эвклидово пространство: $\vec p = \begin{pmatrix} x & y & 0 \end{pmatrix}^T \in \mathbb{P}^2$ – такая точка называется точкой на бесконечности (point at infinity или ideal point).  
При переводе эвклидовых координат в однородные удобно в качестве скаляра, на который умножается вектор, брать 1: $\vec v = \begin{pmatrix} x & y \end{pmatrix}^T \in \mathbb{R}^2 \space \rightarrow \space \vec p = \begin{pmatrix} x & y & 1 \end{pmatrix}^T \in \mathbb{P}^2$.  
Перевод обратно в таком случае также упрощается: $\vec p = \begin{pmatrix} x & y & 1 \end{pmatrix}^T \in \mathbb{P}^2 \space \rightarrow \space \vec v = \begin{pmatrix} x & y \end{pmatrix}^T \in \mathbb{R}^2$.  
Из уроков школьной геометрии мы помним уравнение линии: $y = k \cdot x + b$, где коэффициенты $k$ и $b$ определяют нашу линию. Но удобнее рассматривать линии в другой форме: $a \cdot x + b \cdot y + c = 0$. Можем вывести соотношение этих коэффициентов: $a = k_{line}, \space b = -1, \space c = b_{line}$. А еще удобнее перевести набор коэффициентов в вектор-строку: $l = \begin{pmatrix} a & b & c \end{pmatrix}$.  
Разобравшись с однородными координатами, мы можем теперь рассмотреть свойства точек и линий в этих координатах:  
* Так как $a \cdot x + b \cdot y + c = 0$, то точка лежит на линии. В однородных координатах может записать это так: $l \cdot \vec v = \begin{pmatrix} a & b & c \end{pmatrix} \cdot \begin{pmatrix} x \\ y \\ 1 \end{pmatrix} = 0$.
* Первые две компоненты линии $l$ ($a$ и $b$) - это вектор-перпендикуляр к этой самой линии в эвклидовых координатах. Можно этот вектор нормализовать, тогда компонента $с$ будет равна расстоянию линии до начала координат. А умножая вектор линии на вектор точки, получим величину расстояния от линии до точки: $l = \begin{pmatrix} a & b & c \end{pmatrix}, \vec v = \begin{pmatrix} x & y & 1 \end{pmatrix}^T, \space \space distance(l, \vec v) = (\frac{l}{|\begin{pmatrix} a & b \end{pmatrix}^T|}) \cdot \begin{pmatrix} x & y & 1 \end{pmatrix}^T$. Полученное величина может быть больше или меньше нуля, в зависимости от того, с какой стороны от линии находится точка. 
* Вектор линии $l$ из двух точек $\vec{p_A}$ и $\vec{p_B}$ можно получить их векторным произведением: $l = (\vec{p_A} \times \vec{p_B})^T$. Доказать это просто - в результате векторного произведения получаем ортогональный к двум входным векторам вектор, а ортогональность однородных координат точки и вектора линии озночает, что эта линии пересекает точку: $\vec l \cdot \vec{p_i} = 0$. Значит линия пересекает две входные точки.
* Координаты точки пересечения $\vec p$ двух линий $l_A$ и $l_B$ можно получить их векторным произведением: $\vec{p} = \vec{{l_A}^T} \times \vec{{l_B}^T}$. Доказательство то же - получаем вектор ортогональный к векторам линии, а значит получаенные однородные координаты точки перескают наши две линии. Также если линии параллельны, то $p_w = 0$ - а значит получим точку на бесконечности и не сможем перевести ее в эвклидово пространство. Также вектор $\vec n = \begin{pmatrix} p_x & p_y \end{pmatrix}^T$ - будет перпендикуляром к исходным линиям.  
* Возьмем две точки на бесконечности $\vec{p_A} = \begin{pmatrix} {p_A}_x & {p_A}_y & 0 \end{pmatrix}^T$, $\vec{p_B} = \begin{pmatrix} {p_B}_x & {p_B}_y & 0 \end{pmatrix}^T$ и найдем линию, пересекающую эти две точки: $l = (\vec{p_A} \times \vec{p_B})^T$. В результате получим линию, первые две компоненты вектора которой равны нулю: $l = (\vec{p_A} \times \vec{p_B})^T = \begin{pmatrix} 0 & 0 & {p_A}_x \cdot {p_B}_y - {p_A}_y \cdot {p_B}_x \end{pmatrix}$. Такую линию мы не сможем отобразить в эвклидовом пространстве. Называется она линией на бесконечности (line at infinity). Она не имеет перпендикуляра и пересекается со всеми точками на бесконечности.  

## Трансформации в однородных координатах.  
Все описанные трансформации можно выполнять и для однородных координат.  
Для примера возьмем поворот точек матрицей поворота в эвклидовом пространстве:  $\vec{v'} = R \vec{v} = \begin{pmatrix} R_{11} & R_{12} \\ R_{21} & R_{22} \end{pmatrix} \begin{pmatrix} v_x \\ v_y \end{pmatrix} = \begin{pmatrix} {v'}_x \\ {v'}_y \end{pmatrix}$.  
В однородных координатах это принимает такую форму:  $\vec{v'} = R \vec{v} = \begin{pmatrix} R_{11} & R_{12} & 0 \\ R_{21} & R_{22} & 0 \\ 0 & 0 & 1 \end{pmatrix} \begin{pmatrix} v_x \\ v_y \\ 1 \end{pmatrix} = \begin{pmatrix} {v'}_x \\ {v'}_y \\ 1 \end{pmatrix}$  
В эвклидовых координатах перемещение выполнялось следующим образом: $\vec{v'} = \vec v + \vec t$.  
Смещение объекта - это нелинейная операция в эвклидовом пространстве. Но в проективном пространстве смещение можно реализовать при помощи матрицы $T$:  $\vec{v'} = \begin{pmatrix} 0 & 0 & t_x \\ 0 & 0 & t_y \\ 0 & 0 & 1 \end{pmatrix} \begin{pmatrix} v_x & v_y & 1 \end{pmatrix} = \begin{pmatrix} v_x + t_x \\ v_y + t_y \\ 1 \end{pmatrix}$. Таким образом смещение становится линейной операцией.  
Из предыдущей статьи мы помним, что линейные трансформации, представленные матрицами, мы можем объединять. Объединим поворот и смещение:  
$\vec{v'} = T \cdot R \cdot \vec v = \begin{pmatrix} 0 & 0 & t_x \\ 0 & 0 & t_y \\ 0 & 0 & 1 \end{pmatrix} \cdot \begin{pmatrix} R_{11} & R_{12} & 0 \\ R_{21} & R_{22} & 0 \\ 0 & 0 & 1 \end{pmatrix} \cdot \begin{pmatrix} v_x \\ v_y \\ 1 \end{pmatrix} = \begin{pmatrix} R_{11} & R_{12} & t_x \\ R_{21} & R_{22} & t_y \\ 0 & 0 & 1 \end{pmatrix} \cdot \begin{pmatrix} v_x \\ v_y \\ 1 \end{pmatrix}$  
Пусть $P = T \cdot R$, тогда $\vec{v'} = P \cdot \vec{v}$, где $P = \begin{pmatrix} R_{11} & R_{12} & t_x \\ R_{21} & R_{22} & t_y \\ 0 & 0 & 1 \end{pmatrix}$.  

## Метод наименьших квадратов
Прежде чем двигаться дальше, вооружимся новым инструментом - методом наименьших квадратов (МНК).  
Возьмем для примера такую систему линейных уравнений:  
$\begin{cases} A_{11} \cdot x_1 + A_{12} \cdot x_2 = b_1 \\ A_{21} \cdot x_1 + A_{22} \cdot x_2 = b_2 \\ A_{31} \cdot x_1 + A_{32} \cdot x_1 = b_3\end{cases}$  
Это переопределенная система уравнений, а значит решения одних уравнений может противоречить другим и система может не иметь точного решения. Ошибка получаемых решений - это обычное для вычислительной математики. Важным моментом моментом является выбор функции для вычисления ошибки. Значения $b_i$ нам известен заранее, но от системы уравнений при подставлении значений $x_i$ из-за ошибки получаем отличающиеся значения, которые мы обозначим как ${b'}_i$. Функцию ошибку как разницу суммы квадратов разницы отклонений значений $b_i$: $r(\vec x) = \sum{r_i} = \sum{({b'}_i - b_i)}$.  
Эту систему можно представить в матричном виде:  
$\begin{pmatrix} A_{11} & A_{12} \\ A_{21} & A_{22} \\ A_{31} & A_{32} \end{pmatrix} \cdot \begin{pmatrix} x_1 \\ x_2 \end{pmatrix} = \begin{pmatrix} b_1 \\ b_2 \\ b_3 \end{pmatrix} = A \cdot \vec{x} = \vec{b}$.  
В матричном виде задачу минимизации суммы отклонений можно записать так: $(A \vec x - \vec b)^T (A \vec x - \vec b) \rightarrow min$.  
Решение нашей системы:  
$A \cdot \vec x = \vec b \space \Rightarrow \space A^T \cdot A \cdot \vec x = A^T \vec b \space \Rightarrow \space \vec x = (A^T \cdot A)^{-1} \cdot A^T \cdot \vec b$  
$\vec x = (A^T A)^{-1} A^T \vec b$  
Применение метода несколько шире, чем описанное выше. Мы определили функцию ошибки как сумму квадратов отклонений. А дальше нам нужно найти минимум этой функции. Минимум функции нужно искать в ее экстремумах. Для того, чтобы ее найти, определим систему уравнений из производных функции по каждой свободной переменной приравненных к нулю:  
$\begin{cases}\frac{\partial r(\vec x)}{\partial x_0} = 0 \\ .. \\ \frac{\partial r(\vec x)}{\partial x_n} = 0 \end{cases}$  
Получим систему уравнений, которая в отличии от предыдущей, будет не переопределена. Если система уравнений - линейная, то решение будет только одно и получить его не составляет труда. И что нам важно, оно будет удовлетворять условию, определяемую нашей функцией ошибки: $\sum{r_i} \rightarrow min$.  
Таким образом мы можем использовать этот метода для разных задач со своими условиями.   

## Немного практики
Перейдем наконец к практическим экспериментам. Для того, чтобы упростить себе работу, возьмем все-таки стороннюю библиотеку opencv и используем ее для поиска маркеров. Поставим себе такую задачу - наложим на маркер свою картинку. Математически задачу можно описать так: нужно изменить локальные координаты углов картинки таким образом, чтобы они совпадали с углами маркера на изображении.  
Пусть картинка имеет такие размеры: $w$ - ширина, $h$ - высота. Тогда четыре локальных угла будут иметь следующие координаты:  
$[\vec{v_1}, \vec{v_2}, \vec{v_3}, \vec{v_4}] = [\begin{pmatrix} 0 & 0 \end{pmatrix}^T, \begin{pmatrix} w & 0 \end{pmatrix}^T, \begin{pmatrix} w & h \end{pmatrix}^T, \begin{pmatrix} 0 & h \end{pmatrix}^T]$  
От opencv мы также получаем координаты 4х углов маркера: $[\vec{m_1}, \vec{m_2}, \vec{m_3}, \vec{m_4}]$  
Для того, чтобы описать преобразование из локальных координат картинки в координаты изображения, возьмем две базовых оси, описывающих оси $X$, $Y$ и вектор смещения начала координат. Итого, наше преобразование:  
$\begin{pmatrix} {{m'}_i}_x \\ {{m'}_i}_y \\ 1 \end{pmatrix} = \begin{pmatrix} aX_x & aY_x & t_x \\ aX_y & aY_y & t_y \\ 0 & 0 & 1 \end{pmatrix} \begin{pmatrix} {v_i}_x \\ {v_i}_y \\ 1 \end{pmatrix}$  
Можем упростить формулу, вычеркнув везде последнюю строку:  
$\begin{pmatrix} {{m'}_i}_x \\ {{m'}_i}_y \end{pmatrix} = \begin{pmatrix} aX_x & aY_x & t_x \\ aX_y & aY_y & t_y \end{pmatrix} \begin{pmatrix} {v_i}_x \\ {v_i}_y \end{pmatrix}$  
$\vec{{m'}_i} = \begin{pmatrix} \vec{aX} & \vec{aY} & \vec t \end{pmatrix} \vec{v_i}$  
Одна пара точек задает двое линейных уравнений, при этом имеем 6 неизвестных. Значит нужно 3 пары точек, чтобы система была определена. Мы имеем 4, а значит она переопределена. Воспользуемся МНК для нахождения нашего преобразования:  
Пусть искомый вектор $\vec x$ будет собран из наших неизвестных следующим образом $\vec x = \begin{pmatrix} aX_x & aY_x & t_x & aX_y & aY_y & t_y \end{pmatrix}^T$.
Исходя из такого вектора $\vec x$, матрица $A$, задающая нашу линейную систему уравнений будет иметь следующий вид:  
$A = \begin{pmatrix} {v_1}_x & {v_1}_y & 1 & 0 & 0 & 0 \\ 0 & 0 & 0 & {v_1}_x & {v_1}_y & 1 \\ ... \\ {v_4}_x & {v_4}_y & 1 & 0 & 0 & 0 \\ 0 & 0 & 0 & {v_4}_x & {v_4}_y & 1 \end{pmatrix}$  
А вектор $\vec b$ будет равен:  
$\vec b = \begin{pmatrix} {m_1}_x \\ {m_1}_y \\ ... \\ {m_4}_x \\ {m_4}_y \end{pmatrix}$  
Тогда, используя МНК, можем получить решение нашей системы: $\vec x = (A^T A)^{-1} A^T \vec b$. Отсюда имеем все неизвестные.
Пробуем!
/// Видео
Здесь подкрашиваю красным оригинальный найденный маркер. В целом работает, но если взять низкий угол по отношению к маркеру, то видно, что искажения работают неправильно. А так получилось, потому что для моделирования пространственных искажений маркера мы использовали аффинную матрицу. Важное свойство аффинной матрицы - линии, полученные после преобразования, остаются параллельными. Однако обычно, смотря на параллельные линии на плоскости, мы можем увидеть такую картину:  
![](./assets/Figure_1.png)  
Параллельные линии при проецировании сходятся в одной точке, т.е. становятся не параллельными. Значит аффинной матрицы нам недостаточно.

 ## Direct linear transformation
Получить лучший результат нам поможет алгоритм, который называется Direct linear transformation.  
Попробуем теперь описать наше преобразование полноценной матрицей 3x3: $H = \begin{pmatrix} h_{11} & h_{12} & h_{13} \\ h_{21} & h_{22} & h_{23} \\ h_{31} & h_{32} & h_{33} \end{pmatrix}$.  
Перевод из евклидовых координат в однородные обозначим как $project$ и распишем полученное преобразование для наших $n$ точек ($i = 0...n$, а $n = 4$ в нашем случае):  
$\vec{p_i} = project(H \cdot \begin{pmatrix} {v_i}_x \\ {v_i}_y \\ 1 \end{pmatrix}) = \begin{pmatrix} \frac{ H_{11} \cdot {v_i}_x + H_{12} \cdot {v_i}_y + H_{13}}{H_{31} \cdot {v_i}_x + H_{32} \cdot {v_i}_y + H_{33}} & \frac{ H_{21} \cdot {v_i}_x + H_{22} \cdot {v_i}_y + H_{23}}{H_{31} \cdot {v_i}_x + H_{32} \cdot {v_i}_y + H_{33}} \end{pmatrix}^T$  
Представим наши уравнения в виде системы уравнений:  
$\begin{cases} ... \\ {p_i}_x = \frac{ H_{11} \cdot {v_i}_x + H_{12} \cdot {v_i}_y + H_{13}}{H_{31} \cdot {v_i}_x + H_{32} \cdot {v_i}_y + H_{33}} \\ {p_i}_y = \frac{ H_{21} \cdot {v_i}_x + H_{22} \cdot {v_i}_y + H_{23}}{H_{31} \cdot {v_i}_x + H_{32} \cdot {v_i}_y + H_{33}} \\ .. \end{cases} \Rightarrow \begin{cases} ... \\ {p_i}_x \cdot (H_{31} \cdot {v_i}_x + H_{32} \cdot {v_i}_y + H_{33}) = H_{11} \cdot {v_i}_x + H_{12} \cdot {v_i}_y + H_{13} \\ {p_i}_y \cdot (H_{31} \cdot {v_i}_x + H_{32} \cdot {v_i}_y + H_{33}) = H_{21} \cdot {v_i}_x + H_{22} \cdot {v_i}_y + H_{23} \\ ... \end{cases}$  
 
$\begin{cases} ... \\ H_{11} \cdot v_x + H_{12} \cdot v_y + H_{13} - H_{31} \cdot v_x \cdot {p_i}_x - H_{32} \cdot v_y \cdot {p_i}_x - H_{33} \cdot {p_i}_x = 0 \\ H_{21} \cdot v_x + H_{22} \cdot v_y + H_{23} - H_{31} \cdot v_x \cdot {p_i}_y - H_{32} \cdot v_y \cdot {p_i}_y - H_{33} \cdot {p_i}_y = 0 \\ ... \end{cases}$  

Представим матрицу $H$ как вектор $\vec{h} = \begin{pmatrix} H_{11} & H_{12} & H_{13} & H_{21} & H_{22} & H_{23} & H_{31} & H_{32} & H_{33} \end{pmatrix}^T$.  
Теперь систему уравнений можно перевести в матричный вид: $A = \begin{pmatrix} ... \\ v_x & v_y & 1 & 0 & 0 & 0 & - v_x \cdot {p_i}_x & - v_y \cdot {p_i}_x & - {p_i}_x \\ 0 & 0 & 0 & v_x & v_y & 1 & - v_x \cdot {p_i}_y & - v_y \cdot {p_i}_y & - {p_i}_y \\ ... \end{pmatrix}$  
$A \cdot \vec{h} = \vec{0}$  
Получили однородную систему уравнений в которой нам неизвестен вектор $\vec{h}$ (т.е. матрица $H$). Однородную систему уравнений мы не сможем решить при помощи метода наименьших квадратов. Однако одно решение найти несложно - это нулевой вектор - $\vec{n} = \vec{0}$. Но такое решение нас не интересует. Если же система имеет какое-то ненулевое решение, то мы получаем сразу бесконечное множество решений. Т.е. если мы умножим наш вектор $\vec{h}$ на любой ненулевой скаляр $s$, то система уравнений все равно останется справедливой, а $\vec{h} \cdot s$ - также будет еще одним решением системы: $A \cdot \vec{h} \cdot s = \vec{0}$. Но вспоминаем, что матрица $H$ работает для векторов в однородной системе координат, а эти координаты можно также умножать на любой скаляр, это не повлияет на соответствующие эвклидовы координаты. А значит матрицу $H$ также можно умножать на любой ненулевой скаляр, а отсюда следует, что и вектор $\vec{h}$. Т.е. нас устроит любое решение из полученного множества решений. Т.е. нам не важна длина $\vec{h}$. Тем не менее, чтобы не работать сразу со множеством решений, будем искать только одно - пусть длина вектор будет равна единицы $|\vec{h}| = 1$.  
### Сингулярное разложение и решение однородных систем уравнений 
Решить такую систему уравнений можно при помощи сингулярного разложения матрицы (singular value decomposition). Сингулярное разложение - это разложение вида: $svd(M) = U \cdot W \cdot V^T$, где $U$ и $V$ - ортонормальные матрицы, а $W$ - диагональная, при этом диагональные элементы больше либо равны нулю и располагаются в порядке убывания (сверху вниз). Если воспринимать матрицу как операцию трансформации векторов, то это разложение будет декомпозицией этой трансформации на три последовательных: поворот, масштабирование по осям, второй поворот. Матрицы $U$ и $V$ - это ортонормальные матрицы, а значит можно назвать их матрицами поворота. Только следует учитывать, что $|U| = \pm 1$ и $|V| = \pm 1$, а значит, например, если $M$ - имеет размер 3x3, то тройка базисных векторов и этих матриц поворота могут быть левосторонними.
Не будем останавливаться на том, как вычислять это разложение. В более-менее полноценных математических фреймворках оно будет реализовано. Например - [Eigen](https://eigen.tuxfamily.org/index.php?title=Main_Page).  
Воспользуемся этим разложением для полученной выше матрицы $A$: $svd(A) = U \cdot W \cdot V^T$. Лучшим решением для нашей системы уравнений $A \cdot \vec{h} = \vec{0}$, если $|\vec{h}| = 1$ - это последняя строка матрицы $V$: $\vec{h} = V_{n}$. А так как матрица $V$ - ортонормальная, то длина вектора, составленного из любого его столбца, будет равна как раз единице.  
<details>
    <summary>Доказательство</summary>
    В процессе вычислений у нас всегда есть погрешность, а это значит после $A \cdot \vec{h}$ мы можем получить не нулевой вектор, хотя его компоненты должны быть близки к нулю. Нужно минимизировать получаемую погрешность, а для ее оценки воспользуемся обычной для таких вещей суммой квадратов: $\vec{e} = \sum{{e_i}^2} = \vec{e}^T \vec{e} \rightarrow min$.  
    $\vec{e} = H \cdot \vec{h} \Rightarrow \vec{e}^T \vec{e} = (H \cdot \vec{h})^T (H \cdot \vec{h}) = (U \cdot W \cdot V^T \cdot \vec{h})^T (U \cdot W \cdot V^T \cdot \vec{h}) \rightarrow min$  
    $E = \vec{e}^T \vec{e} = \vec{h}^T \cdot V \cdot W \cdot U^T \cdot U \cdot W \cdot V^T \cdot \vec{h}$, так как $U$ - ортонормальная матрица, то $U^T \cdot U = I$. Так как $W$ - диагональная матрица с ненулевыми элементами, расположенными по убыванию, то и $(W^T \cdot W)$ - также будет диагональной матрицей с ненулевыми элементами, расположенными по убыванию - это будет эквивалентно возведению в квадрат диагональных элементов матрицы. Обозначим $W^2 = W^T \cdot W$.  
    $E = \vec{e}^T \vec{e} = \vec{h}^T \cdot V \cdot W^2 \cdot V^T \cdot \vec{h} \rightarrow min$  
    Обозначим $\vec{c} = V^T \cdot \vec{h}$, заметим, что $V$ - не сохраняет длину вектора $\vec{h}$, значит $|\vec{c}| = 1$:  
    $E = \vec{c}^T \cdot W^2 \cdot \vec{c} \rightarrow min$.
    Представим диагональ матрицы $W$ как вектор $\vec{w}$. Тогда $E = \vec{c}^T \cdot W^2 \cdot \vec{c} = \sum{{c_i}^2 \cdot {w_i}^2} -> min$
    Теперь подумаем чему должен быть равен $\vec{c}$, чтобы $E$ стало минимальным. Так как значения в $\vec{w}$. Судя по $\sum{{c_i}^2 \cdot {w_i}^2}$, наибольший вклад должен вносить последняя компонента вектора $\vec{w}$. Так как $|\vec{c}| = 1$, то выходит $\vec{c} = \begin{pmatrix} 0 & ... & 0 & 1 \end{pmatrix}^T$.  
    $\begin{pmatrix} 0 & ... & 0 & 1 \end{pmatrix}^T = V^T \cdot \vec{h} \Rightarrow \vec{h} = V \cdot \begin{pmatrix} 0 & ... & 0 & 1 \end{pmatrix}^T$.  
    Из описанного выше делаем вывод, что $\vec{h} = V_n$ - это последний столбец матрицы $V$.
</details>

Возвращаемся к нашей системе уравнений $A \cdot \vec{h} = \vec{0}$. Решаем ее описанным выше способом, получаем вектор $\vec{h}$. Затем из этого уже получаем искомую матрицу $H$. Пробуем:  
/// Видео

Как мы видим стало заметно лучше и искажения уже похожи на правду.  
Соответствующий код для аффинных и перспективных искажений можно найти в тестах проекта - [sonar/tests/test_marker_transform](https://github.com/DistinctVision/sonar/tree/main/tests)

## Pinhole camera model

Хорошо, мы получили матрицу, описывающую перевод координат углов маркера в координаты изображения кадра. Но непонятно, что с этой матрицей можно делать. Хотелось бы получить координаты камеры в пространстве, понять где находится камеры относительно наших мировых координат (которые тоже непонятно пока задавать). Сама по себе полученная матрица на эти вопросы не отвечает. А чтобы на эти вопросы все-таки ответить, построим математическую модель, которая будет описывать как точки мирового пространства проецируются на изображение камеры.  
В качестве модели формирования изображения возьмем центральную проекцию. Суть центральной проекции состоит в том, что все точки, которые формируют выходное изображение, формируют лучи, сходящиеся в одной точке - центре проекции. Примерно так лучи себя и ведут в модели глаза или в цифровой камере.  
Проецировать точки будем на плоскость, которая и будет формировать наше изображение. В качестве такой плоскости возьмем плоскость, параллельную оси $Z$, а от нуля сдвинем ее на единицу. Тогда получается, что все точки на этой плоскости имеет координату $z = 1$. При этом наша камера сонаправлена с осью $Z$.  
Думаю станет понятнее, взглянув на картинку:  
